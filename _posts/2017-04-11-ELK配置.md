---
layout:     post
title:      用ELK分析Tomcat日志
subtitle:   配置ELK技术栈来分析apache tomcat日志
date:       2017-05-11
author:     Keith Lee
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - tools
    - shell
---


# 配置ELK技术栈来分析apache tomcat日志
在这篇文章，我将安装ElasticSearch, Logstash and Kibana分析Apache Tomcat服务日志。在安装之前，对各个组件做个简介！

* ElasticSearch
有强大的搜索功能的无模式数据库，可以简单的很想扩展，索引每一个字段，可以聚合分组数据。

* Logstash
用Ruby编写的，我们可以使用管道输入和输出数据到任何位置。一个可以抓取，转换，存储事件到ElasticSearch的ETL管道。打包版本在JRuby上运行，并使用几十个线程进行并行的数据处理，利用了JVM的线程功能。

* Kibana
基于web的数据分析，为ElasticSearch仪表板的工具。充分利用ElasticSearch的搜索功能，以秒为单位可视化数据。支持Lucene的查询字符串的语法和Elasticsearch的过滤功能。

下面，我将开始分别安装技术栈中的每一个组件，下面是步骤：
## 步骤1：

下载并解压 [ElasticSearch.tar.gz](https://www.elastic.co/downloads/elasticsearch) 到一个目录下, 我下载的是 elasticsearch-5.4.0 并且以elasticsearch为文件名解压到指定目录下面

## 步骤2：

在bin目录下以./elasticsearch启动elasticsearch服务，如下：

	$ cd ~/elasticsearch/elasticsearch-5.4.0/bin
	$ ./elasticsearch
上面的命令启动的elasticsearch可以通过 [http://localhost:9200](http://localhost:9200) 访问

## 步骤 3:

下面，我们安装配置[kibana](https://www.elastic.co/downloads/kibana)，指向我们的ElasticSearch实例，同样需要下载[kibana](https://www.elastic.co/downloads/kibana)并解压到指定目录下，我的是kibana-5.4.0-darwin-x64.tar.gz，以kibana为文件名解压到指定目录下
因为上面的操作切换了ruby

## 步骤 4:

修改~/kibana/kibana-5.4.0-darwin-x64/config/kibana.yml配置，指向本地的ElasticSearch实例，替换elasticsearch.url的值为http://localhost:9200

## 步骤 5:

通过bin目录下的./kibana启动kibana，如下：

$ cd ~/kibana/kibana-4.3.0-darwin-x64/bin
$ ./kibana
可以通过 [http://localhost:5601](http://localhost:5601) 访问kibana

## 步骤 6:

下面，我们安装配置Nginx，指向我们的Kibana实例。同样需要下载并解压.tar.gz到目录下，我的是nginx-1.9.6.tar.gz，以nginx为文件名解压到指定目录下，命令如下：

	$ cd nginx-1.9.6
	$ ./configure
	$ make
	$ make install
	
默认情况下，Nginx将被安装到/usr/local/nginx，但是Nginx提供了指定目录安装的方法，使用--prefix选项。如下：

	./configure --prefix=~/nginx
	

下面，我们打开配置文件~/nginx/conf/nginx.conf，然后替换location段中的配置，内容如下：

	location / {
    	# 指向kiban本地实例
    	proxy_pass http://localhost:5601;
    	proxy_http_version 1.1;
    	proxy_set_header Upgrade $http_upgrade;
    	proxy_set_header Connection 'upgrade';
    	proxy_set_header Host $host;
    	proxy_cache_bypass $http_upgrade;
	}


## 步骤 7:

启动 Nginx, 如下:

	cd /Users/ArpitAggarwal/nginx/sbin
	./nginx

可以通过 [http://localhost](http://localhost) 访问nginx

## 步骤 8:

下面，我们安装Logstash，下载并解压 [Logstash.tar.gz](https://www.elastic.co/downloads/logstash) 到一个目录下, 我下载的是 logstash-5.4.0 并且以logstash为文件名解压到指定目录下面


## 步骤 9:

我们需要让Logstash将数据从tomcat server日志目录送到ElasticSearch。创建一个目录，我们将创建logstash配置文件，我将它放到~下，如下：

	cd ~
	mkdir logstash patterns
	cd logstash
	touch logstash.conf
	cd ../patterns
	touch grok-patterns.txt
	
将如下内容复制到：logstash.conf:

	input {
    	file {
        	path => "/Users/ArpitAggarwal/tomcat/logs/*.log*"
        	start_position => beginning
        	type=> "my_log"
    	}
	}
	filter {
    	multiline {
              patterns_dir => "/Users/ArpitAggarwal/logstash/patterns"
              pattern => "\[%{TOMCAT_DATESTAMP}"
              what => "previous"
    }
    if [type] == "my_log"  and "com.test.controller.log.LogController" in [message] {
        mutate {
                add_tag => [ "MY_LOG" ]
               }
        if "_grokparsefailure" in [tags] {
                  drop { }
              }
       date {
             match => [ "timestamp", "UNIX_MS" ]
             target => "@timestamp"
            }
        } else {
            drop { }
      }
	}
	output {
   		stdout {
          codec => rubydebug
   	}
   	if [type] == "my_log"  {
           elasticsearch {
                manage_template => false
                host => localhost
                protocol => http
                port => "9201"
           }
    	}
	}
	
下面，将[patters](https://github.com/elastic/logstash/blob/v1.2.2/patterns/grok-patterns)的内容复制到 patterns/grok-patterns.txt

## 步骤10:
启动Logstash，使数据送到ElasticSearch

	$ cd /usr/local/opt/
	$ logstash -f /Users/ArpitAggarwal/logstash/logstash.conf


> 本文首次发布于 [Keith Lee](http://liliang1314.github.io), 作者 [@李良(Keith)](http://github.com/liliang1314) ,转载请保留原文链接.

